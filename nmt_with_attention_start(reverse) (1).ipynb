{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt_with_attention_start(reverse).ipynb","provenance":[{"file_id":"https://github.com/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb","timestamp":1597474841193}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"26f30a985ed14d66949ac3a6978d6e25":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c03cfc6cffa6471aa8bcc093a9ebd1da","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dd4acc4305974335b87a9c3bb59a3e59","IPY_MODEL_9e403fceab934308b4b70a25602261fd"]}},"c03cfc6cffa6471aa8bcc093a9ebd1da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dd4acc4305974335b87a9c3bb59a3e59":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_0107a9dbebb545128b61ab4238509df7","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":6501,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":6501,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e246a9c52f274408a9d414162179bae4"}},"9e403fceab934308b4b70a25602261fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_8b046d6645fd424cae2356fdd2c7c27a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 6501/6501 [17:41&lt;00:00,  6.13it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_dd85ee331d9842a39906e9cb3a398cbc"}},"0107a9dbebb545128b61ab4238509df7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e246a9c52f274408a9d414162179bae4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8b046d6645fd424cae2356fdd2c7c27a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"dd85ee331d9842a39906e9cb3a398cbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d19190a54f81488b9b15692071a6787f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_8708689c9743486285c8496809b713c1","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_2aedd327ddec4251b21977b29b24a481","IPY_MODEL_218bc6a7f89e43c58cf6cc5e466ff104"]}},"8708689c9743486285c8496809b713c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"2aedd327ddec4251b21977b29b24a481":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_aa2055ca6af24b42aeadb30552a7b133","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":5498,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":5498,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_b3500e9a3c24442c88dcc0528c7970ab"}},"218bc6a7f89e43c58cf6cc5e466ff104":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90aed8df484a4465b890fd9eb501a779","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 5498/5498 [08:06&lt;00:00, 11.30it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_86a41c615d0f4b6d8c87599c674823e2"}},"aa2055ca6af24b42aeadb30552a7b133":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"b3500e9a3c24442c88dcc0528c7970ab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90aed8df484a4465b890fd9eb501a779":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"86a41c615d0f4b6d8c87599c674823e2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"242a4effbc55408185d3a853f21b0933":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_87fdaf4627504611803928d6b7f4f079","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_ab98d48d31ef47d2a0053b33ef229e49","IPY_MODEL_604ff7a68bd149efac109bffe3ed30bb"]}},"87fdaf4627504611803928d6b7f4f079":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab98d48d31ef47d2a0053b33ef229e49":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_189a731d73cf44fa8ef55b061a7682f0","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":11999,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":11999,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7d18b23ac74f415caedff943ad3428b8"}},"604ff7a68bd149efac109bffe3ed30bb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_fbbca1a1daab4b58a03e553e7fd0be3c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 11999/11999 [00:42&lt;00:00, 282.49it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca4b27da784648c884c497035064b740"}},"189a731d73cf44fa8ef55b061a7682f0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7d18b23ac74f415caedff943ad3428b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"fbbca1a1daab4b58a03e553e7fd0be3c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ca4b27da784648c884c497035064b740":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"s_qNSzzyaCbD"},"source":["##### Copyright 2019 The TensorFlow Authors."]},{"cell_type":"code","metadata":{"cellView":"form","colab_type":"code","id":"jmjh290raIky","colab":{}},"source":["#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","# https://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"J0Qjg6vuaHNt"},"source":["# Neural machine translation with attention"]},{"cell_type":"code","metadata":{"id":"Zri7xVrRBZ_g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1597667654432,"user_tz":-540,"elapsed":29140,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"e7b44dae-f3d5-49af-d40a-31f304ca0c74"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tnxXKDjq3jEL","colab":{},"executionInfo":{"status":"ok","timestamp":1597667662078,"user_tz":-540,"elapsed":2374,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import matplotlib.ticker as ticker\n","from sklearn.model_selection import train_test_split\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","import pandas as pd\n","import re"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"MmuTHd2vBjSZ","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597667668915,"user_tz":-540,"elapsed":8990,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["df= pd.read_excel('/content/gdrive/My Drive/kor.xlsx',sheet_name=1)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"QVeZI-EZBoTm","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597667668920,"user_tz":-540,"elapsed":8759,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["# Download the file\n","path_to_file = '/content/gdrive/My Drive/kor.xlsx'"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rd0jw-eC3jEh","colab":{},"executionInfo":{"status":"ok","timestamp":1597667668925,"user_tz":-540,"elapsed":8632,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["# # Converts the unicode file to ascii\n","# def unicode_to_ascii(s):\n","#   return ''.join(c for c in unicodedata.normalize('NFD', s)\n","#       if unicodedata.category(c) != 'Mn')\n","\n","\n","# def preprocess_sentence(w):\n","#   w = w.lower().strip()\n","\n","#   # creating a space between a word and the punctuation following it\n","#   # eg: \"he is a boy.\" => \"he is a boy .\"\n","#   # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","#   w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","#   w = re.sub(r'[\" \"]+', \" \", w)\n","  \n","#   # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","#   w = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", w)\n","#   # w = re.sub(r\"[^ ㄱ-ㅣ가-힣?.!,¿]+\", \" \", w)\n","#   w = w.strip()\n","\n","#   # adding a start and an end token to the sentence\n","#   # so that the model know when to start and stop predicting.\n","#   if len(re.findall('[ㄱ-ㅣ가-힣]+',w)) == 0:\n","#     w = '<start> ' + w + ' <end>'\n","#   return w"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"CZE7d4gMCqJj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597667668927,"user_tz":-540,"elapsed":8514,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["#################################reverse_진행###############################################\n","def preprocess_sentence(w):\n","  w = w.lower().strip()\n","  # creating a space between a word and the punctuation following it\n","  # eg: \"he is a boy.\" => \"he is a boy .\"\n","  # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","  w = re.sub(r'[\" \"]+', \" \", w)\n","  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","  w = re.sub(r\"[^ㄱ-ㅣ가-힣a-zA-Z?.!,]+\", \" \", w)\n","  # w = re.sub(r\"[^ ㄱ-ㅣ가-힣?.!,¿]+\", \" \", w)\n","  w = w.strip()\n","  # adding a start and an end token to the sentence\n","  # so that the model know when to start and stop predicting.\n","  if len(re.findall('[ㄱ-ㅣ가-힣]+',w)) == 0:\n","    w = '<start> ' + w + ' <end>'\n","  else :\n","    # reverse\n","    w = '<start> ' + w[::-1] + ' <end>'\n","    # w = w[::-1]\n","  return w"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"opI2GzOt479E","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597667668928,"user_tz":-540,"elapsed":8292,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"b2444d12-cf0d-49c0-f5d6-0f275f743434"},"source":["ko_sentence = '나는 매일 저녁 배트를 만나러 다락방으로 가요.'\n","en_sentence = \"I go to the attic every evening to meet Bat.\"\n","print(preprocess_sentence(en_sentence))\n","print(preprocess_sentence(ko_sentence))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["<start> i go to the attic every evening to meet bat . <end>\n","<start> . 요가 로으방락다 러나만 를트배 녁저 일매 는나 <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OHn4Dct23jEm","colab":{},"executionInfo":{"status":"ok","timestamp":1597667668929,"user_tz":-540,"elapsed":8147,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["\n","def create_dataset(path, num_examples):\n","  df = pd.read_excel(path,sheet_name=1)\n","  col = df.columns\n","  lines = df[col[1]] +'\\t'+ df[col[2]]\n","  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","\n","  return zip(*word_pairs)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cTbSbBz55QtF","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597667673898,"user_tz":-540,"elapsed":12938,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"e66ea137-be68-47a0-f3ae-1a8440266869"},"source":["en, sp = create_dataset(path_to_file, None)\n","print(en[-1])\n","print(sp[-1])"],"execution_count":9,"outputs":[{"output_type":"stream","text":["<start> . 죠들힘 이많 엔기기즐 를츠포스 이들이아 린어 <end>\n","<start> it is difficult for young children to enjoy sports . <end>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"bIOn8RCNDJXG","colab":{},"executionInfo":{"status":"ok","timestamp":1597667673899,"user_tz":-540,"elapsed":12802,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["def tokenize(lang):\n","  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='',oov_token='<UNK>')\n","  lang_tokenizer.fit_on_texts(lang)\n","\n","  tensor = lang_tokenizer.texts_to_sequences(lang)\n","\n","  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n","                                                         padding='post')\n","\n","  return tensor, lang_tokenizer"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eAY9k49G3jE_","colab":{},"executionInfo":{"status":"ok","timestamp":1597667673900,"user_tz":-540,"elapsed":12671,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["def load_dataset(path, num_examples=None):\n","  # creating cleaned input, output pairs\n","  inp_lang, targ_lang = create_dataset(path, num_examples)\n","\n","  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n","  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n","\n","  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GOi42V79Ydlr"},"source":["### Limit the size of the dataset to experiment faster (optional)\n","\n","Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):"]},{"cell_type":"code","metadata":{"id":"nRo9O9EE7e0d","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597667673901,"user_tz":-540,"elapsed":12327,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["# import pandas as pd\n","# news_df = pd.read_excel('/content/drive/My Drive/B반/data/kor.xlsx', sheet_name='Sheet1')\n","# train_df, val_df, test_df = news_df.iloc[:50000, 1:], news_df.iloc[50000:63000, 1:], news_df.iloc[63000:, 1:]"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cnxC7q-j3jFD","colab":{},"executionInfo":{"status":"ok","timestamp":1597667682285,"user_tz":-540,"elapsed":20321,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["# Try experimenting with the size of that dataset\n","num_examples = 75000\n","input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n","input_tensor, target_tensor = input_tensor[:63000, 1:], target_tensor[:63000, 1:]\n","xtest_tensor = input_tensor[63000:,1:]\n","ytest_tensor =target_tensor[63000:,1:]\n","# Calculate max_length of the target tensors\n","max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"2CxfW9zGG-iG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597667682289,"user_tz":-540,"elapsed":19996,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"1161cbc7-239c-49a9-bd0a-d377a63e718c"},"source":["input_tensor.shape, target_tensor.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((63000, 20), (63000, 20))"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4QILQkOs3jFG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597667682290,"user_tz":-540,"elapsed":19407,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"73d28490-efed-4dae-8653-833cbe13d5ab"},"source":["# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=13000,random_state=42)\n","\n","# Show length\n","print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["50000 50000 13000 13000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"lJPmLZGMeD5q","colab":{},"executionInfo":{"status":"ok","timestamp":1597667682291,"user_tz":-540,"elapsed":18689,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["def convert(lang, tensor):\n","  for t in tensor:\n","    if t!=0:\n","      print (\"%d ----> %s\" % (t, lang.index_word[t]))"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VXukARTDd7MT","colab":{"base_uri":"https://localhost:8080/","height":422},"executionInfo":{"status":"ok","timestamp":1597667682292,"user_tz":-540,"elapsed":18189,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"cb7c2528-82c7-4f1a-f637-98fbaedb9268"},"source":["print (\"Input Language; index to word mapping\")\n","convert(inp_lang, input_tensor_train[0])\n","print ()\n","print (\"Target Language; index to word mapping\")\n","convert(targ_lang, target_tensor_train[0])"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Input Language; index to word mapping\n","4 ----> .\n","1778 ----> 요나만\n","8319 ----> 서에꿈\n","956 ----> 고자\n","48779 ----> 고덮꼭\n","24505 ----> 불이\n","3 ----> <end>\n","\n","Target Language; index to word mapping\n","13451 ----> tucked\n","13 ----> in\n","87 ----> well\n","16 ----> and\n","362 ----> sleep\n","11 ----> ,\n","111 ----> see\n","8 ----> you\n","13 ----> in\n","29 ----> your\n","1336 ----> dreams\n","4 ----> .\n","3 ----> <end>\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rgCLkfv5uO3d"},"source":["### Create a tf.data dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TqHsArVZ3jFS","colab":{},"executionInfo":{"status":"ok","timestamp":1597667688060,"user_tz":-540,"elapsed":22280,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word_index)+1\n","vocab_tar_size = len(targ_lang.word_index)+1\n","\n","dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"qc6-NK1GtWQt","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597667688067,"user_tz":-540,"elapsed":21558,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"e753383a-f938-4dc4-9eb3-38076493057b"},"source":["example_input_batch, example_target_batch = next(iter(dataset))\n","example_input_batch.shape, example_target_batch.shape"],"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(TensorShape([64, 20]), TensorShape([64, 20]))"]},"metadata":{"tags":[]},"execution_count":19}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZ2rI24i3jFg","colab":{},"executionInfo":{"status":"ok","timestamp":1597667688068,"user_tz":-540,"elapsed":20822,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["class Encoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","    super(Encoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.enc_units = enc_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.enc_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","\n","  def call(self, x, hidden):\n","    x = self.embedding(x)\n","    output, state = self.gru(x, initial_state = hidden)\n","    return output, state\n","\n","  def initialize_hidden_state(self):\n","    return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"60gSVh05Jl6l","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597667693125,"user_tz":-540,"elapsed":25245,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"6f55e1a1-c894-4d06-8e17-8b2ff1a899a6"},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","\n","# sample input\n","sample_hidden = encoder.initialize_hidden_state()\n","sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n","print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n","print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Encoder output shape: (batch size, sequence length, units) (64, 20, 1024)\n","Encoder Hidden state shape: (batch size, units) (64, 1024)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"umohpBN2OM94","colab":{},"executionInfo":{"status":"ok","timestamp":1597667693127,"user_tz":-540,"elapsed":24424,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["class BahdanauAttention(tf.keras.layers.Layer):\n","  def __init__(self, units):\n","    super(BahdanauAttention, self).__init__()\n","    self.W1 = tf.keras.layers.Dense(units)\n","    self.W2 = tf.keras.layers.Dense(units)\n","    self.V = tf.keras.layers.Dense(1)\n","\n","  def call(self, query, values):\n","    # query hidden state shape == (batch_size, hidden size)\n","    # query_with_time_axis shape == (batch_size, 1, hidden size)\n","    # values shape == (batch_size, max_len, hidden size)\n","    # we are doing this to broadcast addition along the time axis to calculate the score\n","    query_with_time_axis = tf.expand_dims(query, 1)\n","\n","    # score shape == (batch_size, max_length, 1)\n","    # we get 1 at the last axis because we are applying score to self.V\n","    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n","    score = self.V(tf.nn.tanh(\n","        self.W1(query_with_time_axis) + self.W2(values)))\n","\n","    # attention_weights shape == (batch_size, max_length, 1)\n","    attention_weights = tf.nn.softmax(score, axis=1)\n","\n","    # context_vector shape after sum == (batch_size, hidden_size)\n","    context_vector = attention_weights * values\n","    context_vector = tf.reduce_sum(context_vector, axis=1)\n","\n","    return context_vector, attention_weights"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"k534zTHiDjQU","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597667694680,"user_tz":-540,"elapsed":25250,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"662fb2cc-52a3-4452-8d8f-631eec1e5ea8"},"source":["attention_layer = BahdanauAttention(10)\n","attention_result, attention_weights = attention_layer(sample_hidden, sample_output)\n","\n","print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n","print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"],"execution_count":23,"outputs":[{"output_type":"stream","text":["Attention result shape: (batch size, units) (64, 1024)\n","Attention weights shape: (batch_size, sequence_length, 1) (64, 20, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yJ_B3mhW3jFk","colab":{},"executionInfo":{"status":"ok","timestamp":1597667694682,"user_tz":-540,"elapsed":24568,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["class Decoder(tf.keras.Model):\n","  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","    super(Decoder, self).__init__()\n","    self.batch_sz = batch_sz\n","    self.dec_units = dec_units\n","    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","    self.gru = tf.keras.layers.GRU(self.dec_units,\n","                                   return_sequences=True,\n","                                   return_state=True,\n","                                   recurrent_initializer='glorot_uniform')\n","    self.fc = tf.keras.layers.Dense(vocab_size)\n","\n","    # used for attention\n","    self.attention = BahdanauAttention(self.dec_units)\n","\n","  def call(self, x, hidden, enc_output):\n","    # enc_output shape == (batch_size, max_length, hidden_size)\n","    context_vector, attention_weights = self.attention(hidden, enc_output)\n","\n","    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","    x = self.embedding(x)\n","\n","    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","\n","    # passing the concatenated vector to the GRU\n","    output, state = self.gru(x)\n","\n","    # output shape == (batch_size * 1, hidden_size)\n","    output = tf.reshape(output, (-1, output.shape[2]))\n","\n","    # output shape == (batch_size, vocab)\n","    x = self.fc(output)\n","\n","    return x, state, attention_weights"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P5UY8wko3jFp","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597667694683,"user_tz":-540,"elapsed":23895,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"dd75d81e-aa33-404f-bfc2-d5bbe5a679eb"},"source":["decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n","\n","sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n","                                      sample_hidden, sample_output)\n","\n","print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Decoder output shape: (batch_size, vocab size) (64, 19232)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WmTHr5iV3jFr","colab":{},"executionInfo":{"status":"ok","timestamp":1597667694684,"user_tz":-540,"elapsed":20645,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["optimizer = tf.keras.optimizers.Adam()\n","loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n","    from_logits=True, reduction='none')\n","\n","def loss_function(real, pred):\n","  mask = tf.math.logical_not(tf.math.equal(real, 0))\n","  loss_ = loss_object(real, pred)\n","\n","  mask = tf.cast(mask, dtype=loss_.dtype)\n","  loss_ *= mask\n","\n","  return tf.reduce_mean(loss_)"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DMVWzzsfNl4e"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zj8bXQTgNwrF","colab":{},"executionInfo":{"status":"ok","timestamp":1597667694684,"user_tz":-540,"elapsed":17941,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n","                                 encoder=encoder,\n","                                 decoder=decoder)"],"execution_count":27,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hpObfY22IddU"},"source":["## Training\n","\n","1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n","2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n","3. The decoder returns the *predictions* and the *decoder hidden state*.\n","4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","5. Use *teacher forcing* to decide the next input to the decoder.\n","6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n","7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sC9ArXSsVfqn","colab":{},"executionInfo":{"status":"ok","timestamp":1597667694685,"user_tz":-540,"elapsed":15978,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["@tf.function\n","def train_step(inp, targ, enc_hidden):\n","  loss = 0\n","\n","  with tf.GradientTape() as tape:\n","    enc_output, enc_hidden = encoder(inp, enc_hidden)\n","\n","    dec_hidden = enc_hidden\n","\n","    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n","\n","    # Teacher forcing - feeding the target as the next input\n","    for t in range(1, targ.shape[1]):\n","      # passing enc_output to the decoder\n","      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","\n","      loss += loss_function(targ[:, t], predictions)\n","\n","      # using teacher forcing\n","      dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","  batch_loss = (loss / int(targ.shape[1]))\n","\n","  variables = encoder.trainable_variables + decoder.trainable_variables\n","\n","  gradients = tape.gradient(loss, variables)\n","\n","  optimizer.apply_gradients(zip(gradients, variables))\n","\n","  return batch_loss"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ddefjBMa3jF0","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1597670590673,"user_tz":-540,"elapsed":2882887,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"995a70ec-a743-4a2a-c427-9d567b2b2c53"},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","  start = time.time()\n","\n","  enc_hidden = encoder.initialize_hidden_state()\n","  total_loss = 0\n","\n","  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n","    batch_loss = train_step(inp, targ, enc_hidden)\n","    total_loss += batch_loss\n","\n","    if batch % 100 == 0:\n","      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                   batch,\n","                                                   batch_loss.numpy()))\n","  # saving (checkpoint) the model every 2 epochs\n","  if (epoch + 1) % 2 == 0:\n","    checkpoint.save(file_prefix = checkpoint_prefix)\n","\n","  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                      total_loss / steps_per_epoch))\n","  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":29,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 5.0940\n","Epoch 1 Batch 100 Loss 2.8624\n","Epoch 1 Batch 200 Loss 2.9663\n","Epoch 1 Batch 300 Loss 2.9179\n","Epoch 1 Batch 400 Loss 2.8892\n","Epoch 1 Batch 500 Loss 2.4440\n","Epoch 1 Batch 600 Loss 2.4842\n","Epoch 1 Batch 700 Loss 2.5433\n","Epoch 1 Loss 2.7604\n","Time taken for 1 epoch 305.310898065567 sec\n","\n","Epoch 2 Batch 0 Loss 2.4916\n","Epoch 2 Batch 100 Loss 2.4195\n","Epoch 2 Batch 200 Loss 2.2893\n","Epoch 2 Batch 300 Loss 2.3182\n","Epoch 2 Batch 400 Loss 2.3274\n","Epoch 2 Batch 500 Loss 2.2301\n","Epoch 2 Batch 600 Loss 2.2703\n","Epoch 2 Batch 700 Loss 2.2960\n","Epoch 2 Loss 2.3178\n","Time taken for 1 epoch 287.53005814552307 sec\n","\n","Epoch 3 Batch 0 Loss 2.1808\n","Epoch 3 Batch 100 Loss 2.0597\n","Epoch 3 Batch 200 Loss 2.1422\n","Epoch 3 Batch 300 Loss 2.0543\n","Epoch 3 Batch 400 Loss 2.0468\n","Epoch 3 Batch 500 Loss 1.8853\n","Epoch 3 Batch 600 Loss 2.0331\n","Epoch 3 Batch 700 Loss 2.0593\n","Epoch 3 Loss 2.0727\n","Time taken for 1 epoch 285.0877344608307 sec\n","\n","Epoch 4 Batch 0 Loss 1.8824\n","Epoch 4 Batch 100 Loss 1.9199\n","Epoch 4 Batch 200 Loss 1.8455\n","Epoch 4 Batch 300 Loss 1.7359\n","Epoch 4 Batch 400 Loss 1.8453\n","Epoch 4 Batch 500 Loss 1.7736\n","Epoch 4 Batch 600 Loss 1.7794\n","Epoch 4 Batch 700 Loss 1.8551\n","Epoch 4 Loss 1.8208\n","Time taken for 1 epoch 287.7634913921356 sec\n","\n","Epoch 5 Batch 0 Loss 1.6039\n","Epoch 5 Batch 100 Loss 1.5752\n","Epoch 5 Batch 200 Loss 1.6125\n","Epoch 5 Batch 300 Loss 1.4733\n","Epoch 5 Batch 400 Loss 1.5373\n","Epoch 5 Batch 500 Loss 1.5176\n","Epoch 5 Batch 600 Loss 1.5727\n","Epoch 5 Batch 700 Loss 1.5979\n","Epoch 5 Loss 1.5604\n","Time taken for 1 epoch 285.4752857685089 sec\n","\n","Epoch 6 Batch 0 Loss 1.2295\n","Epoch 6 Batch 100 Loss 1.3672\n","Epoch 6 Batch 200 Loss 1.3922\n","Epoch 6 Batch 300 Loss 1.3777\n","Epoch 6 Batch 400 Loss 1.2588\n","Epoch 6 Batch 500 Loss 1.1677\n","Epoch 6 Batch 600 Loss 1.3010\n","Epoch 6 Batch 700 Loss 1.2254\n","Epoch 6 Loss 1.3005\n","Time taken for 1 epoch 286.98578119277954 sec\n","\n","Epoch 7 Batch 0 Loss 0.9640\n","Epoch 7 Batch 100 Loss 1.0253\n","Epoch 7 Batch 200 Loss 1.0384\n","Epoch 7 Batch 300 Loss 0.9890\n","Epoch 7 Batch 400 Loss 1.0051\n","Epoch 7 Batch 500 Loss 1.0824\n","Epoch 7 Batch 600 Loss 1.0203\n","Epoch 7 Batch 700 Loss 1.0911\n","Epoch 7 Loss 1.0509\n","Time taken for 1 epoch 284.770441532135 sec\n","\n","Epoch 8 Batch 0 Loss 0.7952\n","Epoch 8 Batch 100 Loss 0.8043\n","Epoch 8 Batch 200 Loss 0.8888\n","Epoch 8 Batch 300 Loss 0.8791\n","Epoch 8 Batch 400 Loss 0.8199\n","Epoch 8 Batch 500 Loss 0.8123\n","Epoch 8 Batch 600 Loss 0.8367\n","Epoch 8 Batch 700 Loss 0.8060\n","Epoch 8 Loss 0.8275\n","Time taken for 1 epoch 287.76757621765137 sec\n","\n","Epoch 9 Batch 0 Loss 0.6481\n","Epoch 9 Batch 100 Loss 0.6161\n","Epoch 9 Batch 200 Loss 0.6512\n","Epoch 9 Batch 300 Loss 0.5758\n","Epoch 9 Batch 400 Loss 0.6913\n","Epoch 9 Batch 500 Loss 0.6332\n","Epoch 9 Batch 600 Loss 0.6403\n","Epoch 9 Batch 700 Loss 0.6597\n","Epoch 9 Loss 0.6367\n","Time taken for 1 epoch 284.9678509235382 sec\n","\n","Epoch 10 Batch 0 Loss 0.5092\n","Epoch 10 Batch 100 Loss 0.4485\n","Epoch 10 Batch 200 Loss 0.4778\n","Epoch 10 Batch 300 Loss 0.4913\n","Epoch 10 Batch 400 Loss 0.5163\n","Epoch 10 Batch 500 Loss 0.5202\n","Epoch 10 Batch 600 Loss 0.4995\n","Epoch 10 Batch 700 Loss 0.5280\n","Epoch 10 Loss 0.4835\n","Time taken for 1 epoch 288.1722116470337 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mU3Ce8M6I3rz"},"source":["## Translate\n","\n","* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n","* Stop predicting when the model predicts the *end token*.\n","* And store the *attention weights for every time step*.\n","\n","Note: The encoder output is calculated only once for one input."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EbQpyYs13jF_","colab":{},"executionInfo":{"status":"ok","timestamp":1597670590675,"user_tz":-540,"elapsed":458892,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["def evaluate(sentence):\n","  attention_plot = np.zeros((max_length_targ, max_length_inp))\n","\n","  sentence = preprocess_sentence(sentence)\n","\n","  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n","  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n","                                                         maxlen=max_length_inp,\n","                                                         padding='post')\n","  inputs = tf.convert_to_tensor(inputs)\n","\n","  result = ''\n","\n","  hidden = [tf.zeros((1, units))]\n","  enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","  dec_hidden = enc_hidden\n","  dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n","\n","  for t in range(max_length_targ):\n","    predictions, dec_hidden, attention_weights = decoder(dec_input,\n","                                                         dec_hidden,\n","                                                         enc_out)\n","\n","    # storing the attention weights to plot later on\n","    attention_weights = tf.reshape(attention_weights, (-1, ))\n","    attention_plot[t] = attention_weights.numpy()\n","\n","    predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","    result += targ_lang.index_word[predicted_id] + ' '\n","\n","    if targ_lang.index_word[predicted_id] == '<end>':\n","      return result, sentence, attention_plot\n","\n","    # the predicted ID is fed back into the model\n","    dec_input = tf.expand_dims([predicted_id], 0)\n","\n","  return result, sentence, attention_plot"],"execution_count":30,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5hQWlbN3jGF","colab":{},"executionInfo":{"status":"ok","timestamp":1597670590676,"user_tz":-540,"elapsed":458132,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","  fig = plt.figure(figsize=(10,10))\n","  ax = fig.add_subplot(1, 1, 1)\n","  ax.matshow(attention, cmap='viridis')\n","\n","  fontdict = {'fontsize': 14}\n","\n","  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","  ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n","  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n","\n","  plt.show()"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sl9zUHzg3jGI","colab":{},"executionInfo":{"status":"ok","timestamp":1597670590677,"user_tz":-540,"elapsed":457395,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["def translate(sentence):\n","  result, sentence, attention_plot = evaluate(sentence)\n","\n","  # print('Input: %s' % (sentence))\n","  # print('Predicted translation: {}'.format(result))\n","  return result.replace(' <end>','')\n","\n","  # attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","  # plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UJpT9D5_OgP6","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597670591200,"user_tz":-540,"elapsed":456069,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"d83b56f1-fa92-42fa-b3ee-418be8454c3b"},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd396bd0b00>"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"id":"eW7nsqDZM42z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597670591201,"user_tz":-540,"elapsed":455130,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"bc52fedf-5ca9-44d4-a427-7c6e81aec6ee"},"source":["df['en'].iloc[3]"],"execution_count":34,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"I'm going back to Korea today at midnight.\""]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"code","metadata":{"id":"sYeTlyRA3CmI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597642028817,"user_tz":-540,"elapsed":548,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"9cc2d576-0e12-4cb2-be6d-b1ee77328912"},"source":["df['en'].iloc[11]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'He leaves for Suwon in a few minutes.'"]},"metadata":{"tags":[]},"execution_count":79}]},{"cell_type":"code","metadata":{"id":"9Th2tNLC3G7H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597642018306,"user_tz":-540,"elapsed":559,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"41bd0ed3-99f5-44c4-ab19-e0bea70ef4e2"},"source":["df['ko'].iloc[3]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'나는 오늘 자정에 한국으로 돌아 가요.'"]},"metadata":{"tags":[]},"execution_count":76}]},{"cell_type":"code","metadata":{"id":"mpPhOtrY3MGk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597642032893,"user_tz":-540,"elapsed":546,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"e8bea404-2346-49d7-9944-b3b46234a510"},"source":["df['ko'].iloc[11]\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'그는 조금 있으면 수원으로 가요.'"]},"metadata":{"tags":[]},"execution_count":80}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WrAM0FDomq3E","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597630600476,"user_tz":-540,"elapsed":555,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"cd6cb3b9-ab1d-4a42-d167-25575856f064"},"source":["translate('어린 아이들이 스포츠를 즐기기엔 많이 힘들죠.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["it is difficult for young children to play sports . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zSx2iM36EZQZ","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597642054204,"user_tz":-540,"elapsed":798,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"c6bbbab6-a365-4757-dd46-251e51505c0f"},"source":["translate('나는 오늘 자정에 한국으로 돌아 가요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["i am going back to korea tonight . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3LLCx3ZE0Ls","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597642054550,"user_tz":-540,"elapsed":723,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"1f56697d-e97a-4e73-ffb7-e4ffc5e0b601"},"source":["translate('그는 조금 있으면 수원으로 가요.')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["he leaves for suwon in a little . <end> \n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UeavW0y6NHbk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597647662499,"user_tz":-540,"elapsed":1432,"user":{"displayName":"Haeun Kim","photoUrl":"","userId":"06897618806622586671"}},"outputId":"c83919b0-d1c8-44de-ca96-500c5ba4a046"},"source":["translate('선생님이 밥을 먹으러 갔다.')\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["m going to go to my dad and have a younger brother . <end> \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"32Z7JdhNfEda","colab_type":"text"},"source":["## BLEU SCORE"]},{"cell_type":"code","metadata":{"id":"MHGjijEIbJbN","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597670701004,"user_tz":-540,"elapsed":912,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["from tqdm.notebook import tqdm"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"HMqmjSIpqAW9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":116,"referenced_widgets":["26f30a985ed14d66949ac3a6978d6e25","c03cfc6cffa6471aa8bcc093a9ebd1da","dd4acc4305974335b87a9c3bb59a3e59","9e403fceab934308b4b70a25602261fd","0107a9dbebb545128b61ab4238509df7","e246a9c52f274408a9d414162179bae4","8b046d6645fd424cae2356fdd2c7c27a","dd85ee331d9842a39906e9cb3a398cbc","d19190a54f81488b9b15692071a6787f","8708689c9743486285c8496809b713c1","2aedd327ddec4251b21977b29b24a481","218bc6a7f89e43c58cf6cc5e466ff104","aa2055ca6af24b42aeadb30552a7b133","b3500e9a3c24442c88dcc0528c7970ab","90aed8df484a4465b890fd9eb501a779","86a41c615d0f4b6d8c87599c674823e2"]},"executionInfo":{"status":"ok","timestamp":1597671763942,"user_tz":-540,"elapsed":1062108,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"e0c6001e-2982-4a7b-fda4-fd9f33a4313c"},"source":["ref = []\n","pred = []\n","for i in tqdm(range(63000,69501)):\n","  ref.append(df['en'].iloc[i])\n","  pred.append(translate(df['ko'].iloc[i]))\n","for i in tqdm(range(69501,74999)):\n","  ref.append(df['en'].iloc[i])\n","  pred.append(translate(df['ko'].iloc[i]))"],"execution_count":36,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26f30a985ed14d66949ac3a6978d6e25","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=6501.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d19190a54f81488b9b15692071a6787f","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=5498.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"El7wQ9Xs54fv","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597671905196,"user_tz":-540,"elapsed":885,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}}},"source":["import nltk.translate.bleu_score as bleu\n","from nltk.translate.bleu_score import SmoothingFunction\n","def sentences_to_bleu(ref, pred):\n","  \"\"\"\n","  ref : 참고용 타겟 문장(학습용 영어 문장)\n","  pred : 예측 문장(번역 결과)\n","  \"\"\"\n","  smoothie = SmoothingFunction().method4\n","  return bleu.sentence_bleu(ref,pred, smoothing_function=smoothie)"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXAq_Ulfr1kW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597672221030,"user_tz":-540,"elapsed":1297,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"0b4bb4ef-eb68-46e4-cc6f-65f5904269c8"},"source":["pred[0]"],"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'have forgotten that there was there today . '"]},"metadata":{"tags":[]},"execution_count":46}]},{"cell_type":"code","metadata":{"id":"TcH9dtqEr3I9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1597672229071,"user_tz":-540,"elapsed":961,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"45198c23-3a14-44ee-c851-4f3d8e6a379e"},"source":["ref[0]"],"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"And I'm here to explain the reason for this.\""]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"An14xFjffssO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["242a4effbc55408185d3a853f21b0933","87fdaf4627504611803928d6b7f4f079","ab98d48d31ef47d2a0053b33ef229e49","604ff7a68bd149efac109bffe3ed30bb","189a731d73cf44fa8ef55b061a7682f0","7d18b23ac74f415caedff943ad3428b8","fbbca1a1daab4b58a03e553e7fd0be3c","ca4b27da784648c884c497035064b740"]},"executionInfo":{"status":"ok","timestamp":1597672293595,"user_tz":-540,"elapsed":43099,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"1cbad04c-8c55-4d42-fd60-11862b8b34a5"},"source":["score = []\n","for i in tqdm(range(len(ref))):\n","  score.append(sentences_to_bleu(ref[i],pred[i]))\n","  # score.append(sentences_to_bleu(df['en'].iloc[i],translate(df['ko'].iloc[i])))\n","  # score.append(sentences_to_bleu(ref,pred))\n","  \n","# for i in tqdm(range(69001,75000)):\n","#   score.append(sentences_to_bleu(df['en'].iloc,translate(df['ko'].iloc[i])))\n","BLEU = np.mean(score)"],"execution_count":48,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"242a4effbc55408185d3a853f21b0933","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=11999.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Fz4rslrQVVum","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597672298895,"user_tz":-540,"elapsed":886,"user":{"displayName":"멘띠멘띠","photoUrl":"","userId":"12305161916637742971"}},"outputId":"d142364c-95e7-45dc-88be-dd7e7cbd0d75"},"source":["BLEU"],"execution_count":49,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.31137686954902555"]},"metadata":{"tags":[]},"execution_count":49}]},{"cell_type":"code","metadata":{"id":"Ong-sfuHgSgL","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}